# import pyspark

from pyspark import SparkContext

from pyspark import SparkConf

from pyspark.sql import SQLContext

from pyspark.sql.types import StringType, DoubleType, IntegerType

from pyspark.sql.functions import udf


# import cassandra spark connector(datastax)

from cassandra.cluster import Cluster

from cassandra.query import BatchStatement

from cassandra import ConsistencyLevel

import schema

import config



# def sendCassandra(iter):

#    print("send to cassandra")
#    cluster = Cluster(config.CASSANDRA_SERVER)
#    session = cluster.connect(config.CASSANDRA_NAMESPACE)

#    insert_statement = session.prepare("INSERT INTO  (userid, newsid, count) VALUES (?, ?, ?)")

#    count = 0

    # batch insert into cassandra database
#    batch = BatchStatement(consistency_level=ConsistencyLevel.QUORUM)
    
#    for record in iter:
#        batch.add(insert_statement, (record['userid'], record['newsid'], record['count']))


        # split the batch, so that the batch will not exceed the size limit
#        count += 1
#        if count % 500 == 0:
#            session.execute(batch)
#           batch = BatchStatement(consistency_level=ConsistencyLevel.QUORUM)

    # send the batch that is less than 500            
#    session.execute(batch)
#    session.shutdown()

def reduce_category(cate):
		
	if cate:
		reduced_cate = config.cateMapping[cate]
	else:
		reduced_cate = cate
	
	return reduced_cate


if __name__ == "__main__":

	# Set Spark context
	sc = SparkContext(appName="News-Users-Analytics")
	sc.addPyFile('config.py')
	# Set Spark SQL context
    	sqlContext = SQLContext(sc)

	# Read news data from S3
	gdelt_bucket = "s3a://userclicklogs/news1.txt"

	df = sqlContext.read \
    	.format('com.databricks.spark.csv') \
    	.options(header='false') \
    	.options(delimiter="\t") \
		.load(gdelt_bucket, schema = schema.customSchema)
	
	sqlContext.registerDataFrameAsTable(df, 'temp')

	df_clean = sqlContext.sql("""SELECT newsid,date,Actor1Type1Code,
                               CAST(mentions AS INTEGER)
                          	FROM temp
                         	WHERE Actor1Type1Code <> '' AND Actor1Type1Code IS NOT NULL
                            AND mentions <> '' AND mentions IS NOT NULL""")
	
	sqlContext.dropTempTable('temp')

	df_news = df_clean.select('newsid','date','Actor1Type1Code', 'mentions')

	cateReduction = udf(lambda z: reduce_category(z), StringType())

	df_reduced = df_news.select('newsid','date','Actor1Type1Code', 'mentions',\
				cateReduction('Actor1Type1Code').alias('category'))

	df_filtered = df_reduced.select('newsid','date','category', 'mentions')
	
	df_filtered.write.format("org.apache.spark.sql.cassandra").mode('append').options(
  	table='news', keyspace='playground').save()
	
	df_rep = df_filtered.repartition(10000,'newsid')		
	user_bucket = "s3a://userclicklogs/user.txt"
	# Read user clicklogs from S3
	click_logs = sc.textFile(user_bucket)
	
	# Split lines into columns by delimiter '\t'
	record = click_logs.map(lambda x: x.split("\t"))

	# Convert Rdd into DataFrame
	df_click = sqlContext.createDataFrame(record,['unixtime','userid','newsid'])

	df_user = df_click.repartition(10000,'newsid')

	df_join = df_user.join(df_rep, on='newsid')

	df_grouped = df_join.groupby(['userid', 'category']).count()


	df_grouped.write.format("org.apache.spark.sql.cassandra").mode('append').options(
  	table='demo1', keyspace='playground').save()
